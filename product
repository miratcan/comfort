#!/usr/bin/env python3

import os
import json
import requests
from bs4 import BeautifulSoup
from PIL import Image
from slugify import slugify
from jinja2 import Environment
import click
from urllib.parse import urlparse, urlunparse, urlencode
from dotenv import load_dotenv
import ollama
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load environment variables
load_dotenv()

SCRAPER_API_KEY = os.getenv('SCRAPER_API_KEY')
if not SCRAPER_API_KEY:
    raise RuntimeError('SCRAPER_API_KEY must be defined in .env file or as an environment variable.')

# Constants
ASSETS_FOLDER = "assets"
SOURCE_JSON_FILE = "source.json"
OUTPUT_README = "README.md"
IMG_SIZE = 200

# Create assets folder if it doesn't exist
os.makedirs(ASSETS_FOLDER, exist_ok=True)

def proxied(url):
    """Create a proxied URL for API requests."""
    payload = {'api_key': SCRAPER_API_KEY, 'url': url}
    return 'http://api.scraperapi.com/?' + urlencode(payload)

# Jinja2 template for README.md
README_TEMPLATE = """
{% for product in products %}
<a href="{{ product.url }}" align="left">
    <img width="{{ IMG_SIZE }}" height="{{ IMG_SIZE }}" src="{{ product.img }}" alt="{{ product.name }}">
</a>
{% endfor %}
"""

def clean_product_name(product_name):
    """Shortens the product name using Ollama's model."""
    quest = f"I am owner of a webpage that lists specific products from amazon.com and you're assisting me for content creation. When adding a product, we see that the name of the product is too long for our web page so I want to shorten it. Do not add any explanation to the answer and give me only one result. The name of the product is \"{product_name}\""
    
    try:
        response = ollama.chat(model='llama3.2', messages=[{
            'role': 'user',
            'content': quest
          },
        ])
        return response['message']['content']
    except Exception as e:
        logging.error(f"Failed to clean product name: {e}")
        return product_name  # Fallback to original name in case of failure

def clean_product_url(product_url):
    """Cleans the Amazon product URL by removing any unnecessary parameters."""
    parsed_url = urlparse(product_url)

    if "/dp/" in parsed_url.path:
        path_parts = parsed_url.path.split("/dp/")
        clean_path = f"/dp/{path_parts[1].split('/')[0]}"
    elif "/gp/product/" in parsed_url.path:
        path_parts = parsed_url.path.split("/gp/product/")
        clean_path = f"/gp/product/{path_parts[1].split('/')[0]}"
    else:
        raise ValueError("Invalid Amazon product URL format")

    clean_url = urlunparse((parsed_url.scheme, parsed_url.netloc, clean_path, "", "", ""))
    return clean_url

def fetch_product_details(product_url):
    """Fetches product details from Amazon based on the cleaned URL."""
    clean_url = clean_product_url(product_url)
    response = requests.get(proxied(clean_url))
    
    if response.status_code != 200:
        logging.error(f"Failed to fetch product details: {response.status_code}")
        response.raise_for_status()

    soup = BeautifulSoup(response.content, 'html.parser')
    product_name = soup.find("span", {"id": "productTitle"}).get_text(strip=True)
    product_name = clean_product_name(product_name)
    image_url = soup.find("img", {"id": "landingImage"})["src"]

    asin = clean_url.split("/dp/")[1] if "/dp/" in clean_url else clean_url.split("/gp/product/")[1]
    return product_name, image_url, asin, clean_url

def download_and_process_image(image_url, product_name, asin):
    """Downloads and processes the product image."""
    try:
        response = requests.get(image_url)
        response.raise_for_status()

        # Open the image and resize it while maintaining the aspect ratio
        image = Image.open(requests.get(image_url, stream=True).raw)
        image.thumbnail((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)  # Resize keeping aspect ratio

        # Create a new square image and paste the resized image onto it
        new_image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (255, 255, 255))  # White background
        new_image.paste(image, ((IMG_SIZE - image.width) // 2, (IMG_SIZE - image.height) // 2))

        filename = f"{slugify(product_name)}-{asin}.jpg"
        image_path = os.path.join(ASSETS_FOLDER, filename)

        new_image.save(image_path, "JPEG")
        return image_path
    except Exception as e:
        logging.error(f"Failed to download and process image: {e}")
        return None

def update_source_json(product_name, image_path, clean_url):
    """Updates the source.json file with new product details."""
    # Load existing data or initialize a new list if the file doesn't exist
    try:
        with open(SOURCE_JSON_FILE, "r") as f:
            data = json.load(f)
    except (FileNotFoundError, json.decoder.JSONDecodeError):
        data = []

    # Check if the product already exists to avoid duplicates
    for entry in data:
        if entry["url"] == clean_url:
            logging.warning("Product already exists in source.json. No new entry created.")
            return

    # Add the new product to the data
    data.append({"img": image_path, "name": product_name, "url": clean_url})

    # Write the updated data back to source.json
    with open(SOURCE_JSON_FILE, "w") as f:
        json.dump(data, f, indent=4)

def remove_product_from_json(clean_url):
    """Removes a product entry from source.json and deletes the associated image file."""
    with open(SOURCE_JSON_FILE, "r") as f:
        data = json.load(f)

    new_data = [entry for entry in data if entry["url"] != clean_url]

    if len(new_data) == len(data):
        logging.warning("Product URL not found in source.json.")
        return

    # Remove the image associated with the product
    for entry in data:
        if entry["url"] == clean_url:
            image_path = entry["img"]
            if os.path.exists(image_path):
                os.remove(image_path)
                logging.info(f"Deleted image: {image_path}")
            break

    # Write the updated data back to source.json
    with open(SOURCE_JSON_FILE, "w") as f:
        json.dump(new_data, f, indent=4)

    logging.info("Product removed successfully from source.json.")

def build_readme():
    """Builds README.md from the product data in source.json."""
    with open(SOURCE_JSON_FILE, "r") as f:
        data = json.load(f)

    env = Environment()
    template = env.from_string(README_TEMPLATE)

    readme_content = template.render(products=data, IMG_SIZE=IMG_SIZE)

    with open(OUTPUT_README, "w") as f:
        f.write(readme_content)

def clean_unused_assets():
    """Removes images from the assets folder that are not referenced in source.json."""
    with open(SOURCE_JSON_FILE, "r") as f:
        data = json.load(f)

    used_images = {entry["img"] for entry in data}
    for filename in os.listdir(ASSETS_FOLDER):
        file_path = os.path.join(ASSETS_FOLDER, filename)
        if file_path not in used_images:
            os.remove(file_path)
            logging.info(f"Deleted unused asset: {file_path}")

@click.command()
@click.option("--add", "operation", flag_value="add", help="Add a new product by URL.")
@click.option("--build", "operation", flag_value="build", help="Build README.md from source.json.")
@click.option("--rm", "operation", flag_value="rm", help="Remove a product by URL.")
@click.option("--clean", "operation", flag_value="clean", help="Remove unused assets from the assets folder.")
@click.argument("product_url", required=False)
def main(operation, product_url):
    """Main function for managing products."""
    if operation == "add":
        if not product_url:
            logging.error("Error: product_url is required for the 'add' option.")
            return
        
        product_name, image_url, asin, clean_url = fetch_product_details(product_url)
        image_path = download_and_process_image(image_url, product_name, asin)

        if image_path:
            update_source_json(product_name, image_path, clean_url)
            logging.info(f"Product '{product_name}' added successfully with image '{image_path}'.")

    elif operation == "build":
        build_readme()
        logging.info("README.md built successfully.")

    elif operation == "rm":
        if not product_url:
            logging.error("Error: product_url is required for the 'rm' option.")
            return
        
        clean_url = clean_product_url(product_url)
        remove_product_from_json(clean_url)

    elif operation == "clean":
        clean_unused_assets()
        logging.info("Unused assets cleaned successfully.")

if __name__ == "__main__":
    main()

